{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid2op.Agent import BaseAgent\n",
    "from grid2op.Converter.IdToAct import IdToAct\n",
    "\n",
    "\n",
    "\n",
    "from grid2op.multi_agent.ma_typing import LocalObservation, LocalObservationSpace, \\\n",
    "    LocalAction, LocalActionSpace \n",
    "\n",
    "from grid2op import make\n",
    "from grid2op.Action.PlayableAction import PlayableAction\n",
    "from grid2op.Action import BaseAction\n",
    "from grid2op.multi_agent.multiAgentEnv import MultiAgentEnv\n",
    "import numpy as np\n",
    "from grid2op.multi_agent.multi_agentExceptions import *\n",
    "\n",
    "    \n",
    "import numpy as np\n",
    "\n",
    "from lightsim2grid import LightSimBackend\n",
    "bk_cls = LightSimBackend\n",
    "\n",
    "action_domains = {\n",
    "    'agent_0' : [0,1,2,3, 4],\n",
    "    'agent_1' : [5,6,7,8,9,10,11,12,13]\n",
    "}\n",
    "#env_name = \"l2rpn_case14_sandbox\"#\"educ_case14_storage\"\n",
    "#env = make(env_name, test=False, backend = bk_cls(),\n",
    "#                action_class=PlayableAction, _add_to_name=\"_test_ma\")\n",
    "#\n",
    "#\n",
    "#ma_env = MultiAgentEnv(env, action_domains, copy_env=False)\n",
    "#\n",
    "#ma_env.seed(0)\n",
    "#obs = ma_env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Env wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_tested_action(action_space):\n",
    "    res = [action_space({})]  # add the do nothing\n",
    "    # better use \"get_all_unitary_topologies_set\" and not \"get_all_unitary_topologies_change\"\n",
    "    # maybe \"change\" are still \"bugged\" (in the sens they don't count all topologies exactly once)\n",
    "    res += action_space.get_all_unitary_topologies_set(action_space)\n",
    "    return res\n",
    "\n",
    "#ALL_ACTIONS_SINGLE = _get_tested_action(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "class EnvWrapper(gym.Env):\n",
    "    def __init__(self, env_config=None):\n",
    "        env_name = \"l2rpn_case14_sandbox\"#\"educ_case14_storage\"\n",
    "        self.env = make(env_name, test=False, backend = bk_cls(),\n",
    "                        action_class=PlayableAction, _add_to_name=\"_test_ma\")\n",
    "        self.env.seed(0)\n",
    "        self.all_actions = self._get_tested_action(self.env.action_space)\n",
    "        obs = self.env.reset().to_vect()\n",
    "        \n",
    "        self.observation_space = Box(shape=obs.shape, high=np.infty, low=-np.infty)#GymObservationSpace(self.env)\n",
    "        self.action_space = Discrete(len(self.all_actions))\n",
    "        \n",
    "        self.reward_range = self.env.reward_range\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        return obs.to_vect()\n",
    "    \n",
    "    def step(self, action):\n",
    "        a = self.all_actions[action]\n",
    "        \n",
    "        obs, r, info, done = self.env.step(a)\n",
    "        \n",
    "        return obs.to_vect(), r, done, info\n",
    "    \n",
    "    def _get_tested_action(self, action_space):\n",
    "        res = [action_space({})]  # add the do nothing\n",
    "        # better use \"get_all_unitary_topologies_set\" and not \"get_all_unitary_topologies_change\"\n",
    "        # maybe \"change\" are still \"bugged\" (in the sens they don't count all topologies exactly once)\n",
    "        res += action_space.get_all_unitary_topologies_set(action_space)\n",
    "        return res\n",
    "    \n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv as MAEnv\n",
    "\n",
    "class MAEnvWrapper(MAEnv):\n",
    "    def __init__(self, env_config=None):\n",
    "        super().__init__\n",
    "        \n",
    "        env_name = \"l2rpn_case14_sandbox\"#\"educ_case14_storage\"\n",
    "        env = make(env_name, test=False, backend = bk_cls(),\n",
    "                action_class=PlayableAction, _add_to_name=\"_test_ma\")\n",
    "\n",
    "\n",
    "        self.ma_env = MultiAgentEnv(env, action_domains, copy_env=False)\n",
    "        self._agent_ids = set(self.ma_env.agents)\n",
    "        self.ma_env.seed(0)\n",
    "        obs = self.ma_env.reset()[self.ma_env.agents[0]].to_vect()\n",
    "        self.observation_space = Box(shape=obs.shape, high=np.infty, low=-np.infty)#{\n",
    "        #    agent : Box(shape=obs.shape, high=np.infty, low=-np.infty)\n",
    "        #    for agent in self.ma_env.agents\n",
    "        #}\n",
    "        \n",
    "        self.all_actions = {\n",
    "            agent : self._get_tested_action(self.ma_env.action_spaces[agent])\n",
    "            for agent in self.ma_env.agents\n",
    "        }\n",
    "        self.action_space = {\n",
    "            agent : Discrete(len(self.all_actions[agent]))\n",
    "            for agent in self.ma_env.agents\n",
    "        }\n",
    "        \n",
    "    def reset(self):\n",
    "        obs = self.ma_env.reset()\n",
    "        o = obs[self.ma_env.agents[0]].to_vect()\n",
    "        return {\n",
    "            agent : o.copy()\n",
    "            for agent in self.ma_env.agents\n",
    "        }\n",
    "    \n",
    "    def step(self, actions):\n",
    "        a = {\n",
    "            agent : self.all_actions[agent][actions[agent]]\n",
    "            for agent in self.ma_env.agents\n",
    "        }\n",
    "        \n",
    "        obs, r, info, done = self.ma_env.step(a)\n",
    "        done['__all__'] = done[self.ma_env.agents[0]]\n",
    "        #info['__all__'] = ''\n",
    "        o = obs[self.ma_env.agents[0]].to_vect()\n",
    "        obs = {\n",
    "            agent : o.copy()\n",
    "            for agent in self.ma_env.agents\n",
    "        }\n",
    "        info = dict()\n",
    "        return obs, r, done, info\n",
    "    \n",
    "    def _get_tested_action(self, action_space):\n",
    "        res = [action_space({})]  # add the do nothing\n",
    "        # better use \"get_all_unitary_topologies_set\" and not \"get_all_unitary_topologies_change\"\n",
    "        # maybe \"change\" are still \"bugged\" (in the sens they don't count all topologies exactly once)\n",
    "        res += action_space.get_all_unitary_topologies_set(action_space)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/najarfar/Internship/Grid2Op/grid2op/multi_agent/multiAgentEnv.py:123: UserWarning: Rules can not be changed in this version.\n",
      "  warnings.warn(\"Rules can not be changed in this version.\")\n",
      "/home/najarfar/Internship/Grid2Op/grid2op/multi_agent/multiAgentEnv.py:129: UserWarning: The central env has been heavily modified (parameters and reset) !\n",
      "  warnings.warn(\"The central env has been heavily modified (parameters and reset) !\")\n"
     ]
    }
   ],
   "source": [
    "new_env = EnvWrapper()\n",
    "new_ma_env = MAEnvWrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.55     |\n",
      "|    ep_rew_mean     | 153      |\n",
      "| time/              |          |\n",
      "|    fps             | 28       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 71       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f7adcf3b910>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "model = PPO(\"MlpPolicy\", new_env, verbose=1)\n",
    "model.learn(total_timesteps=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard URL: http://\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2215429/1968686887.py:19: DeprecationWarning: Accessing values through ctx[\"webui_url\"] is deprecated. Use ctx.address_info[\"webui_url\"] instead.\n",
      "  print(\"Dashboard URL: http://{}\".format(info[\"webui_url\"]))\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.agents.ppo import ppo\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "checkpoint_root = \"./single_ppo\"\n",
    "# Where checkpoints are written:\n",
    "shutil.rmtree(checkpoint_root, ignore_errors=True, onerror=None)\n",
    "\n",
    "# Where some data will be written and used by Tensorboard below:\n",
    "ray_results = f'{os.getenv(\"HOME\")}/ray_results/'\n",
    "shutil.rmtree(ray_results, ignore_errors=True, onerror=None)\n",
    "\n",
    "info = ray.init(ignore_reinit_error=True)\n",
    "print(\"Dashboard URL: http://{}\".format(info[\"webui_url\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT_ENV = EnvWrapper                            # Specifies the OpenAI Gym environment for Cart Pole\n",
    "N_ITER = 10                                     # Number of training runs.\n",
    "\n",
    "config = ppo.DEFAULT_CONFIG.copy()              # PPO's default configuration. See the next code cell.\n",
    "config[\"log_level\"] = \"WARN\"                    # Suppress too many messages, but try \"INFO\" to see what can be printed.\n",
    "\n",
    "# Other settings we might adjust:\n",
    "config[\"num_workers\"] = 1                       # Use > 1 for using more CPU cores, including over a cluster\n",
    "config[\"num_sgd_iter\"] = 10                     # Number of SGD (stochastic gradient descent) iterations per training minibatch.\n",
    "                                                # I.e., for each minibatch of data, do this many passes over it to train. \n",
    "config[\"sgd_minibatch_size\"] = 64              # The amount of data records per minibatch\n",
    "config[\"model\"][\"fcnet_hiddens\"] = [100, 50]    #\n",
    "config[\"num_cpus_per_worker\"] = 0  # This avoids running out of resources in the notebook environment when this cell is re-executed\n",
    "\n",
    "#config['disable_env_checking']=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-23 10:05:34,936\tINFO trainer.py:2332 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "2022-08-23 10:05:34,938\tINFO ppo.py:414 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-08-23 10:05:34,939\tINFO trainer.py:903 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2210590)\u001b[0m 2022-08-23 10:05:44,346\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2022-08-23 10:05:46,170\tINFO trainable.py:159 -- Trainable.setup took 11.236 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-08-23 10:05:46,171\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "2022-08-23 10:08:05,709\tWARNING deprecation.py:46 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n",
      "2022-08-23 10:08:06,368\tWARNING ppo.py:494 -- The mean reward returned from the environment is 43.16822814941406 but the vf_clip_param is set to 10.0. Consider increasing it for policy: default_policy to improve value function convergence.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: Min/Mean/Max reward: -10.0000/153.8404/844.6186. Checkpoint saved to ./single_ppo/cart/checkpoint_000001/checkpoint-1\n",
      "  1: Min/Mean/Max reward: -10.0000/179.9672/1109.2118. Checkpoint saved to ./single_ppo/cart/checkpoint_000002/checkpoint-2\n",
      "  2: Min/Mean/Max reward: -10.0000/205.8721/996.2866. Checkpoint saved to ./single_ppo/cart/checkpoint_000003/checkpoint-3\n",
      "  3: Min/Mean/Max reward: -10.0000/238.8013/1270.9819. Checkpoint saved to ./single_ppo/cart/checkpoint_000004/checkpoint-4\n",
      "  4: Min/Mean/Max reward: -10.0000/267.5639/1759.7718. Checkpoint saved to ./single_ppo/cart/checkpoint_000005/checkpoint-5\n",
      "  5: Min/Mean/Max reward: -10.0000/301.4468/1499.2796. Checkpoint saved to ./single_ppo/cart/checkpoint_000006/checkpoint-6\n",
      "  6: Min/Mean/Max reward: -10.0000/336.7569/1938.4094. Checkpoint saved to ./single_ppo/cart/checkpoint_000007/checkpoint-7\n",
      "  7: Min/Mean/Max reward: -10.0000/363.0247/2188.7150. Checkpoint saved to ./single_ppo/cart/checkpoint_000008/checkpoint-8\n",
      "  8: Min/Mean/Max reward: -10.0000/400.7207/1591.1799. Checkpoint saved to ./single_ppo/cart/checkpoint_000009/checkpoint-9\n",
      "  9: Min/Mean/Max reward: -10.0000/434.8692/2201.7313. Checkpoint saved to ./single_ppo/cart/checkpoint_000010/checkpoint-10\n"
     ]
    }
   ],
   "source": [
    "agent = ppo.PPOTrainer(config, env=SELECT_ENV)\n",
    "\n",
    "results = []\n",
    "episode_data = []\n",
    "episode_json = []\n",
    "\n",
    "for n in range(N_ITER):\n",
    "    result = agent.train()\n",
    "    results.append(result)\n",
    "    \n",
    "    episode = {'n': n, \n",
    "               'episode_reward_min': result['episode_reward_min'], \n",
    "               'episode_reward_mean': result['episode_reward_mean'], \n",
    "               'episode_reward_max': result['episode_reward_max'],  \n",
    "               'episode_len_mean': result['episode_len_mean']}\n",
    "    \n",
    "    episode_data.append(episode)\n",
    "    episode_json.append(json.dumps(episode))\n",
    "    file_name = agent.save(checkpoint_root)\n",
    "    \n",
    "    print(f'{n:3d}: Min/Mean/Max reward: {result[\"episode_reward_min\"]:8.4f}/{result[\"episode_reward_mean\"]:8.4f}/{result[\"episode_reward_max\"]:8.4f}. Checkpoint saved to {file_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('single_ppo/rewards.json', 'w') as outfile:\n",
    "    json.dump(episode_json, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-23 11:32:04,924\tINFO worker.py:973 -- Calling ray.init() again after it has already been called.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard URL: http://\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2218030)\u001b[0m /home/najarfar/Internship/Grid2Op/grid2op/multi_agent/multiAgentEnv.py:123: UserWarning: Rules can not be changed in this version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2218030)\u001b[0m   warnings.warn(\"Rules can not be changed in this version.\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2218030)\u001b[0m /home/najarfar/Internship/Grid2Op/grid2op/multi_agent/multiAgentEnv.py:129: UserWarning: The central env has been heavily modified (parameters and reset) !\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2218030)\u001b[0m   warnings.warn(\"The central env has been heavily modified (parameters and reset) !\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2218030)\u001b[0m 2022-08-23 11:32:14,341\tWARNING env.py:216 -- Your MultiAgentEnv <MAEnvWrapper instance> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__` from within your MutiAgentEnv's constructor. This will raise an error in the future.\n",
      "2022-08-23 11:32:17,379\tINFO trainable.py:159 -- Trainable.setup took 12.438 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-08-23 11:32:17,381\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/najarfar/ray_results/PPOTrainer_MAEnvWrapper_2022-08-23_11-32-04wbk31c1q/progress.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/najarfar/Internship/Grid2Op/grid2op/multi_agent/analyse/rl_test.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/najarfar/Internship/Grid2Op/grid2op/multi_agent/analyse/rl_test.ipynb#ch0000011?line=54'>55</a>\u001b[0m episode_json \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/najarfar/Internship/Grid2Op/grid2op/multi_agent/analyse/rl_test.ipynb#ch0000011?line=56'>57</a>\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(N_ITER):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/najarfar/Internship/Grid2Op/grid2op/multi_agent/analyse/rl_test.ipynb#ch0000011?line=57'>58</a>\u001b[0m     result \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/najarfar/Internship/Grid2Op/grid2op/multi_agent/analyse/rl_test.ipynb#ch0000011?line=58'>59</a>\u001b[0m     results\u001b[39m.\u001b[39mappend(result)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/najarfar/Internship/Grid2Op/grid2op/multi_agent/analyse/rl_test.ipynb#ch0000011?line=60'>61</a>\u001b[0m     episode \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mn\u001b[39m\u001b[39m'\u001b[39m: n, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/najarfar/Internship/Grid2Op/grid2op/multi_agent/analyse/rl_test.ipynb#ch0000011?line=61'>62</a>\u001b[0m                \u001b[39m'\u001b[39m\u001b[39mepisode_reward_min\u001b[39m\u001b[39m'\u001b[39m: result[\u001b[39m'\u001b[39m\u001b[39mepisode_reward_min\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/najarfar/Internship/Grid2Op/grid2op/multi_agent/analyse/rl_test.ipynb#ch0000011?line=62'>63</a>\u001b[0m                \u001b[39m'\u001b[39m\u001b[39mepisode_reward_mean\u001b[39m\u001b[39m'\u001b[39m: result[\u001b[39m'\u001b[39m\u001b[39mepisode_reward_mean\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/najarfar/Internship/Grid2Op/grid2op/multi_agent/analyse/rl_test.ipynb#ch0000011?line=63'>64</a>\u001b[0m                \u001b[39m'\u001b[39m\u001b[39mepisode_reward_max\u001b[39m\u001b[39m'\u001b[39m: result[\u001b[39m'\u001b[39m\u001b[39mepisode_reward_max\u001b[39m\u001b[39m'\u001b[39m],  \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/najarfar/Internship/Grid2Op/grid2op/multi_agent/analyse/rl_test.ipynb#ch0000011?line=64'>65</a>\u001b[0m                \u001b[39m'\u001b[39m\u001b[39mepisode_len_mean\u001b[39m\u001b[39m'\u001b[39m: result[\u001b[39m'\u001b[39m\u001b[39mepisode_len_mean\u001b[39m\u001b[39m'\u001b[39m]}\n",
      "File \u001b[0;32m~/anaconda3/envs/grid2op/lib/python3.9/site-packages/ray/tune/trainable.py:410\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/najarfar/anaconda3/envs/grid2op/lib/python3.9/site-packages/ray/tune/trainable.py?line=406'>407</a>\u001b[0m \u001b[39mif\u001b[39;00m monitor_data:\n\u001b[1;32m    <a href='file:///home/najarfar/anaconda3/envs/grid2op/lib/python3.9/site-packages/ray/tune/trainable.py?line=407'>408</a>\u001b[0m     result\u001b[39m.\u001b[39mupdate(monitor_data)\n\u001b[0;32m--> <a href='file:///home/najarfar/anaconda3/envs/grid2op/lib/python3.9/site-packages/ray/tune/trainable.py?line=409'>410</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlog_result(result)\n\u001b[1;32m    <a href='file:///home/najarfar/anaconda3/envs/grid2op/lib/python3.9/site-packages/ray/tune/trainable.py?line=411'>412</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stdout_context:\n\u001b[1;32m    <a href='file:///home/najarfar/anaconda3/envs/grid2op/lib/python3.9/site-packages/ray/tune/trainable.py?line=412'>413</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stdout_stream\u001b[39m.\u001b[39mflush()\n",
      "File \u001b[0;32m~/anaconda3/envs/grid2op/lib/python3.9/site-packages/ray/rllib/agents/trainer.py:2078\u001b[0m, in \u001b[0;36mTrainer.log_result\u001b[0;34m(self, result)\u001b[0m\n\u001b[1;32m   <a href='file:///home/najarfar/anaconda3/envs/grid2op/lib/python3.9/site-packages/ray/rllib/agents/trainer.py?line=2075'>2076</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mon_train_result(trainer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, result\u001b[39m=\u001b[39mresult)\n\u001b[1;32m   <a href='file:///home/najarfar/anaconda3/envs/grid2op/lib/python3.9/site-packages/ray/rllib/agents/trainer.py?line=2076'>2077</a>\u001b[0m \u001b[39m# Then log according to Trainable's logging logic.\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/najarfar/anaconda3/envs/grid2op/lib/python3.9/site-packages/ray/rllib/agents/trainer.py?line=2077'>2078</a>\u001b[0m Trainable\u001b[39m.\u001b[39;49mlog_result(\u001b[39mself\u001b[39;49m, result)\n",
      "File \u001b[0;32m~/anaconda3/envs/grid2op/lib/python3.9/site-packages/ray/tune/trainable.py:1019\u001b[0m, in \u001b[0;36mTrainable.log_result\u001b[0;34m(self, result)\u001b[0m\n\u001b[1;32m   <a href='file:///home/najarfar/anaconda3/envs/grid2op/lib/python3.9/site-packages/ray/tune/trainable.py?line=1006'>1007</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlog_result\u001b[39m(\u001b[39mself\u001b[39m, result: Dict):\n\u001b[1;32m   <a href='file:///home/najarfar/anaconda3/envs/grid2op/lib/python3.9/site-packages/ray/tune/trainable.py?line=1007'>1008</a>\u001b[0m     \u001b[39m\"\"\"Subclasses can optionally override this to customize logging.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/najarfar/anaconda3/envs/grid2op/lib/python3.9/site-packages/ray/tune/trainable.py?line=1008'>1009</a>\u001b[0m \n\u001b[1;32m   <a href='file:///home/najarfar/anaconda3/envs/grid2op/lib/python3.9/site-packages/ray/tune/trainable.py?line=1009'>1010</a>\u001b[0m \u001b[39m    The logging here is done on the worker process rather than\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///home/najarfar/anaconda3/envs/grid2op/lib/python3.9/site-packages/ray/tune/trainable.py?line=1016'>1017</a>\u001b[0m \u001b[39m        result: Training result returned by step().\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/najarfar/anaconda3/envs/grid2op/lib/python3.9/site-packages/ray/tune/trainable.py?line=1017'>1018</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/najarfar/anaconda3/envs/grid2op/lib/python3.9/site-packages/ray/tune/trainable.py?line=1018'>1019</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_result_logger\u001b[39m.\u001b[39;49mon_result(result)\n",
      "File \u001b[0;32m~/anaconda3/envs/grid2op/lib/python3.9/site-packages/ray/tune/logger.py:342\u001b[0m, in \u001b[0;36mUnifiedLogger.on_result\u001b[0;34m(self, result)\u001b[0m\n\u001b[1;32m    <a href='file:///home/najarfar/anaconda3/envs/grid2op/lib/python3.9/site-packages/ray/tune/logger.py?line=339'>340</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_result\u001b[39m(\u001b[39mself\u001b[39m, result):\n\u001b[1;32m    <a href='file:///home/najarfar/anaconda3/envs/grid2op/lib/python3.9/site-packages/ray/tune/logger.py?line=340'>341</a>\u001b[0m     \u001b[39mfor\u001b[39;00m _logger \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loggers:\n\u001b[0;32m--> <a href='file:///home/najarfar/anaconda3/envs/grid2op/lib/python3.9/site-packages/ray/tune/logger.py?line=341'>342</a>\u001b[0m         _logger\u001b[39m.\u001b[39;49mon_result(result)\n",
      "File \u001b[0;32m~/anaconda3/envs/grid2op/lib/python3.9/site-packages/ray/tune/logger.py:147\u001b[0m, in \u001b[0;36mCSVLogger.on_result\u001b[0;34m(self, result)\u001b[0m\n\u001b[1;32m    <a href='file:///home/najarfar/anaconda3/envs/grid2op/lib/python3.9/site-packages/ray/tune/logger.py?line=145'>146</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_result\u001b[39m(\u001b[39mself\u001b[39m, result: Dict):\n\u001b[0;32m--> <a href='file:///home/najarfar/anaconda3/envs/grid2op/lib/python3.9/site-packages/ray/tune/logger.py?line=146'>147</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_init()\n\u001b[1;32m    <a href='file:///home/najarfar/anaconda3/envs/grid2op/lib/python3.9/site-packages/ray/tune/logger.py?line=148'>149</a>\u001b[0m     tmp \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    <a href='file:///home/najarfar/anaconda3/envs/grid2op/lib/python3.9/site-packages/ray/tune/logger.py?line=149'>150</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mconfig\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m tmp:\n",
      "File \u001b[0;32m~/anaconda3/envs/grid2op/lib/python3.9/site-packages/ray/tune/logger.py:142\u001b[0m, in \u001b[0;36mCSVLogger._maybe_init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/najarfar/anaconda3/envs/grid2op/lib/python3.9/site-packages/ray/tune/logger.py?line=137'>138</a>\u001b[0m progress_file \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogdir, EXPR_PROGRESS_FILE)\n\u001b[1;32m    <a href='file:///home/najarfar/anaconda3/envs/grid2op/lib/python3.9/site-packages/ray/tune/logger.py?line=138'>139</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_continuing \u001b[39m=\u001b[39m (\n\u001b[1;32m    <a href='file:///home/najarfar/anaconda3/envs/grid2op/lib/python3.9/site-packages/ray/tune/logger.py?line=139'>140</a>\u001b[0m     os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(progress_file) \u001b[39mand\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mgetsize(progress_file) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    <a href='file:///home/najarfar/anaconda3/envs/grid2op/lib/python3.9/site-packages/ray/tune/logger.py?line=140'>141</a>\u001b[0m )\n\u001b[0;32m--> <a href='file:///home/najarfar/anaconda3/envs/grid2op/lib/python3.9/site-packages/ray/tune/logger.py?line=141'>142</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_file \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(progress_file, \u001b[39m\"\u001b[39;49m\u001b[39ma\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    <a href='file:///home/najarfar/anaconda3/envs/grid2op/lib/python3.9/site-packages/ray/tune/logger.py?line=142'>143</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_csv_out \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/najarfar/anaconda3/envs/grid2op/lib/python3.9/site-packages/ray/tune/logger.py?line=143'>144</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initialized \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/najarfar/ray_results/PPOTrainer_MAEnvWrapper_2022-08-23_11-32-04wbk31c1q/progress.csv'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.agents.ppo import ppo\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "checkpoint_root = \"./ma_ppo\"\n",
    "# Where checkpoints are written:\n",
    "shutil.rmtree(checkpoint_root, ignore_errors=True, onerror=None)\n",
    "\n",
    "# Where some data will be written and used by Tensorboard below:\n",
    "ray_results = f'{os.getenv(\"HOME\")}/ray_results/'\n",
    "shutil.rmtree(ray_results, ignore_errors=True, onerror=None)\n",
    "\n",
    "info = ray.init(ignore_reinit_error=True)\n",
    "print(\"Dashboard URL: http://{}\".format(info[\"webui_url\"]))\n",
    "\n",
    "#Configs\n",
    "SELECT_ENV = MAEnvWrapper                            # Specifies the OpenAI Gym environment for Cart Pole\n",
    "N_ITER = 10                                     # Number of training runs.\n",
    "\n",
    "config = ppo.DEFAULT_CONFIG.copy()              # PPO's default configuration. See the next code cell.\n",
    "config[\"log_level\"] = \"WARN\"                    # Suppress too many messages, but try \"INFO\" to see what can be printed.\n",
    "\n",
    "# Other settings we might adjust:\n",
    "config[\"num_workers\"] = 1                       # Use > 1 for using more CPU cores, including over a cluster\n",
    "config[\"num_sgd_iter\"] = 10                     # Number of SGD (stochastic gradient descent) iterations per training minibatch.\n",
    "                                                # I.e., for each minibatch of data, do this many passes over it to train. \n",
    "config[\"sgd_minibatch_size\"] = 64              # The amount of data records per minibatch\n",
    "config[\"model\"][\"fcnet_hiddens\"] = [100, 50]    #\n",
    "config[\"num_cpus_per_worker\"] = 0  # This avoids running out of resources in the notebook environment when this cell is re-executed\n",
    "\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "config[\"multiagent\"] = {\n",
    "    \"policies\" : {\n",
    "        \"agent_0\" : PolicySpec(\n",
    "            action_space=Discrete(len(new_ma_env.all_actions[\"agent_0\"]))\n",
    "        ),\n",
    "        \"agent_1\" : PolicySpec(\n",
    "            action_space=Discrete(len(new_ma_env.all_actions[\"agent_1\"]))\n",
    "        )\n",
    "    },\n",
    "    \"policy_mapping_fn\":\n",
    "            lambda agent_id, episode, worker, **kwargs : agent_id\n",
    "}\n",
    "\n",
    "#Trainer\n",
    "agent = ppo.PPOTrainer(config, env=SELECT_ENV)\n",
    "\n",
    "results = []\n",
    "episode_data = []\n",
    "episode_json = []\n",
    "\n",
    "for n in range(N_ITER):\n",
    "    result = agent.train()\n",
    "    results.append(result)\n",
    "    \n",
    "    episode = {'n': n, \n",
    "               'episode_reward_min': result['episode_reward_min'], \n",
    "               'episode_reward_mean': result['episode_reward_mean'], \n",
    "               'episode_reward_max': result['episode_reward_max'],  \n",
    "               'episode_len_mean': result['episode_len_mean']}\n",
    "    \n",
    "    episode_data.append(episode)\n",
    "    episode_json.append(json.dumps(episode))\n",
    "    file_name = agent.save(checkpoint_root)\n",
    "    \n",
    "    print(f'{n:3d}: Min/Mean/Max reward: {result[\"episode_reward_min\"]:8.4f}/{result[\"episode_reward_mean\"]:8.4f}/{result[\"episode_reward_max\"]:8.4f}. Checkpoint saved to {file_name}')\n",
    "\n",
    "    with open('ma_ppo/rewards.json', 'w') as outfile:\n",
    "        json.dump(episode_json, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e9695fee56864081dd9787bed9cb2ecf5768f301e19b28b0d4bc6bbab594eacc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('grid2op')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
