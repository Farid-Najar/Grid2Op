{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experts that use simulate\n",
    "In this notebook we'll analyze the behaviour of experts that use the `simulate` functionality in the observations to choose actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/najarfar/Internship/Grid2Op/grid2op/multi_agent/multiAgentEnv.py:123: UserWarning: Rules can not be changed in this version.\n",
      "  warnings.warn(\"Rules can not be changed in this version.\")\n",
      "/home/najarfar/Internship/Grid2Op/grid2op/multi_agent/multiAgentEnv.py:129: UserWarning: The central env has been heavily modified (parameters and reset) !\n",
      "  warnings.warn(\"The central env has been heavily modified (parameters and reset) !\")\n"
     ]
    }
   ],
   "source": [
    "from grid2op.Agent import BaseAgent\n",
    "from grid2op.Converter.IdToAct import IdToAct\n",
    "\n",
    "from grid2op.multi_agent.ma_typing import LocalObservation, LocalObservationSpace, \\\n",
    "    LocalAction, LocalActionSpace \n",
    "\n",
    "from grid2op import make\n",
    "from grid2op.Action.PlayableAction import PlayableAction\n",
    "from grid2op.Action import BaseAction\n",
    "from grid2op.multi_agent.multiAgentEnv import MultiAgentEnv\n",
    "import numpy as np\n",
    "from grid2op.multi_agent.multi_agentExceptions import *\n",
    "\n",
    "    \n",
    "import numpy as np\n",
    "\n",
    "from lightsim2grid import LightSimBackend\n",
    "bk_cls = LightSimBackend\n",
    "\n",
    "action_domains = {\n",
    "    'agent_0' : [0,1,2,3, 4],\n",
    "    'agent_1' : [5,6,7,8,9,10,11,12,13]\n",
    "}\n",
    "env_name = \"l2rpn_case14_sandbox\"#\"educ_case14_storage\"\n",
    "env = make(env_name, test=False, backend = bk_cls(),\n",
    "                action_class=PlayableAction, _add_to_name=\"_test_ma\")\n",
    "\n",
    "\n",
    "ma_env = MultiAgentEnv(env, action_domains, copy_env=False)\n",
    "\n",
    "ma_env.seed(0)\n",
    "obs = ma_env.reset()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Dict\n",
    "from grid2op.multi_agent.ma_typing import MAAgents\n",
    "from grid2op.Environment.BaseEnv import BaseEnv\n",
    "from grid2op.Agent.baseAgent import BaseAgent\n",
    "from grid2op.multi_agent.multiAgentEnv import MultiAgentEnv\n",
    "\n",
    "\n",
    "def _run_simple_actor(\n",
    "    env : BaseEnv,\n",
    "    actor : BaseAgent,\n",
    "    nb_episodes : int,\n",
    "    save_path : str\n",
    ") -> dict:\n",
    "    \n",
    "    T = np.zeros(nb_episodes, dtype = int)\n",
    "    obs = env.reset()\n",
    "    t = 0\n",
    "    \n",
    "    print(\"Running simple simulation !\")\n",
    "    from tqdm.notebook import tqdm\n",
    "    \n",
    "    rewards_history = [[] for _ in range(nb_episodes)]\n",
    "    mean_rewards_history = np.zeros(nb_episodes)\n",
    "    std_rewards_history = np.zeros(nb_episodes)\n",
    "    cumulative_reward = np.zeros(nb_episodes)\n",
    "    \n",
    "    info_history = [[] for _ in range(nb_episodes)]\n",
    "    \n",
    "    obs_history = [[] for _ in range(nb_episodes)]\n",
    "    obs_history[0].append(obs.to_vect())\n",
    "    \n",
    "    done_history = [[] for _ in range(nb_episodes)]\n",
    "    \n",
    "    actions_history = [[] for _ in range(nb_episodes)]\n",
    "    \n",
    "    reward = 0\n",
    "    \n",
    "    for episode in tqdm(range(nb_episodes)):\n",
    "        \n",
    "        while True:\n",
    "            t += 1\n",
    "            obs_history[episode].append(obs.to_vect())\n",
    "            \n",
    "            action = actor.act(observation = obs, reward = reward)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            \n",
    "            #obs._obs_env = None\n",
    "            rewards_history[episode].append(reward)\n",
    "            info_history[episode].append(info.copy())\n",
    "            done_history[episode].append(done)\n",
    "            actions_history[episode].append(action.copy())\n",
    "\n",
    "            if done:\n",
    "                #mean_rewards_history[episode] = np.mean(rewards_history)\n",
    "                #std_rewards_history[episode] = np.std(rewards_history)\n",
    "                cumulative_reward[episode] = np.sum(rewards_history[episode])\n",
    "                obs = env.reset()\n",
    "                T[episode] = t\n",
    "                t = 0\n",
    "                \n",
    "                break\n",
    "        if (episode+1)%10==0:\n",
    "            np.save(save_path+'/'+f'single_cum_rewards{episode}.npy', arr=cumulative_reward)\n",
    "            np.save(save_path+'/'+f'single_T{episode}.npy', arr=T)\n",
    "            \n",
    "    return {\n",
    "        'rewards' : rewards_history,\n",
    "        #'mean_rewards' : mean_rewards_history,\n",
    "        #'std_rewards' : std_rewards_history,\n",
    "        'episode_len' : T,\n",
    "        'info_history' : info_history,\n",
    "        'observations' : obs_history,\n",
    "        'done_history' : done_history,\n",
    "        'actions' : actions_history,\n",
    "        'cumulative_reward' : cumulative_reward\n",
    "    }\n",
    "\n",
    "def _run_ma_actors(\n",
    "    ma_env : MultiAgentEnv,\n",
    "    actors : MAAgents,\n",
    "    nb_episodes : int,\n",
    "    save_path : str\n",
    ") -> dict:\n",
    "    \n",
    "    print(\"Running multi-agent simulation !\")\n",
    "    \n",
    "    T = np.zeros(nb_episodes, dtype = int)\n",
    "    obs = ma_env.reset()\n",
    "    t = 0\n",
    "    \n",
    "    from tqdm.notebook import tqdm\n",
    "    \n",
    "    rewards_history = [[] for _ in range(nb_episodes)]\n",
    "    mean_rewards_history = np.zeros(nb_episodes)\n",
    "    std_rewards_history = np.zeros(nb_episodes)\n",
    "    cumulative_reward = np.zeros(nb_episodes)\n",
    "    \n",
    "    info_history = [[] for _ in range(nb_episodes)]\n",
    "    \n",
    "    local_actions = [[] for _ in range(nb_episodes)]\n",
    "    \n",
    "    done_history = [[] for _ in range(nb_episodes)]\n",
    "    \n",
    "    actions_history = [[] for _ in range(nb_episodes)]\n",
    "    \n",
    "    obs_history = [[] for _ in range(nb_episodes)]\n",
    "    \n",
    "    r = 0\n",
    "    \n",
    "    for episode in tqdm(range(nb_episodes)):\n",
    "        while True:\n",
    "            t += 1\n",
    "            \n",
    "            obs_history[episode].append(obs[ma_env.agents[0]].to_vect())\n",
    "            \n",
    "            actions = {\n",
    "                agent : actors[agent].act(observation = obs[agent], reward = r)\n",
    "                for agent in ma_env.agents\n",
    "            }\n",
    "            obs, reward, dones, info = ma_env.step(actions)\n",
    "\n",
    "            r = reward[ma_env.agents[0]]\n",
    "            rewards_history[episode].append(r)\n",
    "            info_history[episode].append(info[ma_env.agents[0]].copy())\n",
    "            \n",
    "            #for agent in ma_env.agents:\n",
    "            #    # TODO pourquoi ce problÃ¨me ?\n",
    "            #    obs[agent]._obs_env = None\n",
    "                \n",
    "            local_actions[episode].append(actions.copy())\n",
    "            done_history[episode].append(dones[ma_env.agents[0]])\n",
    "            actions_history[episode].append(ma_env.global_action.copy())\n",
    "                \n",
    "\n",
    "            if dones[ma_env.agents[0]]:\n",
    "                #mean_rewards_history[episode] = np.mean(rewards_history)\n",
    "                #std_rewards_history[episode] =  np.std(rewards_history)\n",
    "                cumulative_reward[episode] = np.sum(rewards_history[episode])\n",
    "                \n",
    "                obs = ma_env.reset()\n",
    "                T[episode] = t\n",
    "                t = 0\n",
    "                break\n",
    "        if (episode+1)%10 == 0 and episode>0:\n",
    "            np.save(save_path+'/'+f'cum_rewards{episode}.npy', arr=cumulative_reward)\n",
    "            np.save(save_path+'/'+f'T{episode}.npy', arr=T)\n",
    "            np.save(save_path+'/'+f'observations{episode}.npy', arr=obs_history)\n",
    "            np.save(save_path+'/'+f'local_actions{episode}.npy', arr=local_actions)\n",
    "            \n",
    "            \n",
    "    return {\n",
    "        'rewards' : rewards_history,\n",
    "        #'mean_rewards' : mean_rewards_history,\n",
    "        #'std_rewards' : std_rewards_history,\n",
    "        'observations' : obs_history,\n",
    "        'episode_len' : T,\n",
    "        'info_history' : info_history,\n",
    "        'local_actions' : local_actions,\n",
    "        'done_history' : done_history,\n",
    "        'actions' : actions_history,\n",
    "        'cumulative_reward' : cumulative_reward\n",
    "    }\n",
    "\n",
    "    \n",
    "def compare_simple_and_multi(\n",
    "    ma_env, # It is grid2op.multi_agent.multiAgentEnv.MultiAgentEnv\n",
    "    simple_actor : BaseAgent, \n",
    "    ma_actors : MAAgents, \n",
    "    episodes : int = 2,\n",
    "    seed = 0,\n",
    "    chronics_id = 0,\n",
    "    save_path = \"./\",\n",
    "    ):\n",
    "    \n",
    "    ma_env.seed(seed)\n",
    "    ma_env._cent_env.set_id(chronics_id)\n",
    "    \n",
    "    results_simple = _run_simple_actor(ma_env._cent_env, simple_actor, episodes, save_path)\n",
    "    np.save(save_path+'/'+f'results_simple{episodes}.npy', arr=results_simple)\n",
    "    \n",
    "    \n",
    "    ma_env.seed(seed)\n",
    "    ma_env._cent_env.set_id(chronics_id)\n",
    "    results_ma = _run_ma_actors(ma_env, ma_actors, episodes, save_path)\n",
    "    np.save(save_path+'/'+f'results_ma{episodes}.npy', arr=results_ma)\n",
    "    \n",
    "    #save results\n",
    "    # TODO\n",
    "    \n",
    "    return results_simple, results_ma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictors\n",
    "Here we define predictors than given an observation (local or global), it predicts the coworker's action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    def __init__(self, \n",
    "                 action_space,\n",
    "                 do_nothing = False,\n",
    "                 model = None,\n",
    "                 nn_kwargs = {}):\n",
    "        \n",
    "        self.action_space = action_space\n",
    "        self.do_nothing = do_nothing\n",
    "        self.model = model\n",
    "        \n",
    "        res = [self.action_space({})]  # add the do nothing\n",
    "        res += self.action_space.get_all_unitary_topologies_set(self.action_space)\n",
    "        self.all_actions = res\n",
    "        \n",
    "    \n",
    "    def predict(self, observation : LocalObservation) -> LocalAction:\n",
    "        if self.do_nothing:\n",
    "            return self.action_space({})\n",
    "        elif self.model is not None:\n",
    "            #TODO return the prediction\n",
    "            a = self.model.predict([observation.to_vect()])[0]\n",
    "            return self.all_actions[a]\n",
    "        else:\n",
    "            raise(\"Model is missing !\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local greedy expert with topological actions only\n",
    "Here, local experts aim to find the best topological action using the simulate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "import copy\n",
    "\n",
    "class LocalTopologyGreedyExpert(BaseAgent):\n",
    "    def __init__(self,\n",
    "                 agent_nm,\n",
    "                 action_space,\n",
    "                 ma_env,\n",
    "                 predictors : Optional[List[Predictor]] = None,\n",
    "                 **kwargs):\n",
    "        super().__init__(action_space)\n",
    "        \n",
    "        self.agent_nm = agent_nm\n",
    "        self.other_agents = [\n",
    "            agent\n",
    "            for agent in ma_env.agents if agent != agent_nm\n",
    "        ]\n",
    "        self.executed_actions = set()\n",
    "        \n",
    "        self.action_space = action_space\n",
    "        \n",
    "        self.curr_iter = 0\n",
    "        self.ma_env = ma_env\n",
    "        \n",
    "        self.global_action_space = ma_env._cent_env.action_space\n",
    "        \n",
    "        if predictors is None:\n",
    "            self.predictors = {\n",
    "                agent : Predictor(ma_env.action_spaces[agent], do_nothing=True)\n",
    "                for agent in self.other_agents\n",
    "            }\n",
    "        else:\n",
    "            self.predictors = predictors\n",
    "            \n",
    "        self.tested_action = None\n",
    "            \n",
    "    def act(self, observation : LocalObservation, reward, done = False):\n",
    "        self.curr_iter += 1\n",
    "\n",
    "        # Look for overloads and rank them\n",
    "        #ltc_list = self.getRankedOverloads(observation)\n",
    "        #counterTestedOverloads = 0\n",
    "        #overloaded = np.any(observation.rho >= 1)\n",
    "        #\n",
    "        #if not overloaded:\n",
    "        #    return self.action_space({})\n",
    "        #else:\n",
    "        other_actions = {\n",
    "            agent : self.predictors[agent].predict(observation)\n",
    "            for agent in self.other_agents\n",
    "        }\n",
    "        \n",
    "        self.tested_action = self._get_tested_action(observation)\n",
    "        if len(self.tested_action) > 1:\n",
    "            self.resulting_rewards = np.full(\n",
    "                shape=len(self.tested_action), fill_value=-np.infty, dtype=float\n",
    "            )\n",
    "            for i, action in enumerate(self.tested_action):\n",
    "                actions = copy.deepcopy(other_actions)\n",
    "                actions.update({self.agent_nm : action})\n",
    "                a = action.to_global(self.ma_env._cent_env.action_space)\n",
    "                is_legal, reason = self.ma_env._cent_env._game_rules(action=a, env=self.ma_env._cent_env)\n",
    "                if is_legal:\n",
    "                    (\n",
    "                        simul_obs,\n",
    "                        simul_reward,\n",
    "                        simul_has_error,\n",
    "                        simul_info,\n",
    "                    ) = observation.simulate(actions)\n",
    "                    self.resulting_rewards[i] = simul_reward\n",
    "                else:\n",
    "                    self.resulting_rewards[i] = -np.infty\n",
    "\n",
    "            reward_idx = int(\n",
    "                np.argmax(self.resulting_rewards)\n",
    "            )\n",
    "            \n",
    "            best_action = self.tested_action[reward_idx]\n",
    "        else:\n",
    "            best_action = self.tested_action[0]\n",
    "            \n",
    "        return best_action\n",
    "        \n",
    "    def _get_tested_action(self, observation):\n",
    "        if self.tested_action is None:\n",
    "            res = [self.action_space({})]  # add the do nothing\n",
    "            # better use \"get_all_unitary_topologies_set\" and not \"get_all_unitary_topologies_change\"\n",
    "            # maybe \"change\" are still \"bugged\" (in the sens they don't count all topologies exactly once)\n",
    "            res += self.action_space.get_all_unitary_topologies_set(self.action_space)\n",
    "            self.tested_action = res\n",
    "        return self.tested_action\n",
    "            \n",
    "    def reset(self, observation):\n",
    "        # No internal states to reset\n",
    "        pass\n",
    "\n",
    "    def load(self, path):\n",
    "        # Nothing to load\n",
    "        pass\n",
    "\n",
    "    def save(self, path):\n",
    "        # Nothing to save\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assuming others do nothing\n",
    "In this section, each agent assumes that other agents don't act on the grid. Hence, it has to do the best it can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running simple simulation !\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd87e8e066d40c3a9cb629f017d3f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running multi-agent simulation !\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa8d4e350ede4b42a7bfe82f0f43772b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/najarfar/anaconda3/envs/grid2op/lib/python3.9/site-packages/numpy/lib/npyio.py:518: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.asanyarray(arr)\n",
      "/home/najarfar/anaconda3/envs/grid2op/lib/python3.9/site-packages/numpy/lib/npyio.py:518: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.asanyarray(arr)\n",
      "/home/najarfar/anaconda3/envs/grid2op/lib/python3.9/site-packages/numpy/lib/npyio.py:518: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.asanyarray(arr)\n",
      "/home/najarfar/anaconda3/envs/grid2op/lib/python3.9/site-packages/numpy/lib/npyio.py:518: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.asanyarray(arr)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "from grid2op.Agent import TopologyGreedy, DoNothingAgent\n",
    "\n",
    "simple_actor = DoNothingAgent(env.action_space)\n",
    "episodes = 50\n",
    "ma_actors = dict()\n",
    "for agent_nm in ma_env.agents:\n",
    "    ma_actors[agent_nm] = LocalTopologyGreedyExpert(\n",
    "        agent_nm,\n",
    "        ma_env.action_spaces[agent_nm],\n",
    "        ma_env\n",
    "    )\n",
    "\n",
    "\n",
    "results_simple, results_ma = compare_simple_and_multi(\n",
    "    ma_env,\n",
    "    simple_actor=simple_actor,\n",
    "    ma_actors=ma_actors,\n",
    "    episodes=episodes,\n",
    "    save_path=f'./results{episodes}'\n",
    "    # TODO plus d'episodes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(episodes):\n",
    "    assert (results_simple['observations'][episode][0] == results_ma['observations'][episode][0]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/najarfar/Internship/Grid2Op/grid2op/multi_agent/analyse/experts_w_simulate.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/najarfar/Internship/Grid2Op/grid2op/multi_agent/analyse/experts_w_simulate.ipynb#ch0000011?line=0'>1</a>\u001b[0m np\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39mresults_ma_l2rpn100.npy\u001b[39m\u001b[39m'\u001b[39m, arr\u001b[39m=\u001b[39mresults_ma)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/najarfar/Internship/Grid2Op/grid2op/multi_agent/analyse/experts_w_simulate.ipynb#ch0000011?line=1'>2</a>\u001b[0m np\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39mresults_do_nothing_l2rpn100.npy\u001b[39m\u001b[39m'\u001b[39m, arr\u001b[39m=\u001b[39mresults_simple)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "np.save('results_ma_l2rpn100.npy', arr=results_ma)\n",
    "np.save('results_do_nothing_l2rpn100.npy', arr=results_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_ma' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/najarfar/Internship/Grid2Op/grid2op/multi_agent/analyse/experts_w_simulate.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/najarfar/Internship/Grid2Op/grid2op/multi_agent/analyse/experts_w_simulate.ipynb#ch0000048?line=0'>1</a>\u001b[0m results_ma[\u001b[39m'\u001b[39m\u001b[39mcumulative_rewards\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results_ma' is not defined"
     ]
    }
   ],
   "source": [
    "results_ma['cumulative_rewards']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.load('results_ma_l2rpn.npy', allow_pickle=True)[None][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "episodes = len(res['info_history'])\n",
    "\n",
    "is_illegal_ma = [[] for _ in range(episodes)]\n",
    "\n",
    "reason_illegal_ma = [[] for _ in range(episodes)]\n",
    "\n",
    "for episode in range(episodes):\n",
    "    for info in res['info_history'][episode]:\n",
    "        is_illegal_ma[episode].append(info['action_is_illegal'])\n",
    "        if is_illegal_ma[episode][-1]:\n",
    "            reason_illegal_ma[episode].append(info['reason_illegal'])\n",
    "print(np.concatenate(is_illegal_ma).sum())#np.concatenate(res['local_actions'])[np.concatenate(is_illegal_ma)][1]['agent_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running simple simulation !\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15dd06ad39ff400eaf5c7b8ff452cd5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ma_env.seed(0)\n",
    "ma_env._cent_env.set_id(0)\n",
    "simple_actor = TopologyGreedy(ma_env._cent_env.action_space)\n",
    "\n",
    "results_simple1 = _run_simple_actor(ma_env._cent_env, simple_actor, 100)\n",
    "np.save(\"results_simple_l2rpn100.npy\", arr=results_simple1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid2op.Agent.fromActionsListAgent import FromActionsListAgent\n",
    "\n",
    "one_agent_replays = []\n",
    "replays = []\n",
    "for episode in range(episodes):\n",
    "    # Faire plusieurs replay sur plusieurs episodes\n",
    "    replays.append(\n",
    "        FromActionsListAgent(\n",
    "            env.action_space,\n",
    "            results_ma['actions'][episode]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    one_agent_replays.append(\n",
    "        FromActionsListAgent(\n",
    "            env.action_space,\n",
    "            results_simple['actions'][episode]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_ma['cumulative_reward'] - results_simple['cumulative_reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illegal_hist = [results_ma['info_history'][0][i][\"is_illegal\"] for i in range(len(results_ma['info_history']))]\n",
    "np.where(illegal_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_ma['episode_len'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in results_ma['actions'][0]:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_names = ['simple', 'ma']\n",
    "\n",
    "plt.boxplot([results_simple['cumulative_reward'], results_ma['cumulative_reward']])\n",
    "plt.xticks([1, 2], box_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_names = ['simple', 'ma']\n",
    "\n",
    "plt.boxplot([results_simple['episode_len'], results_ma['episode_len']])\n",
    "plt.xticks([1, 2], box_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid2op.Agent import DoNothingAgent\n",
    "env = ma_env._cent_env\n",
    "env.seed(0)\n",
    "env.set_id(0)\n",
    "\n",
    "results_do_nothing = _run_simple_actor(env, actor=DoNothingAgent(env.action_space), nb_episodes=episodes)\n",
    "rs_donothing = np.mean(results_do_nothing['rewards'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_ma = []\n",
    "rs_simple = []\n",
    "for episode in range(episodes):\n",
    "    rs_ma += list(results_ma['rewards'][episode])\n",
    "    rs_simple += list(results_simple['rewards'][episode])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_ma = np.array(rs_ma)#np.load('rewards_ma_educ_storage2.npy')\n",
    "rs_simple = np.array(rs_simple)#np.load('rewards_simple_educ_storage2.npy')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize = (21,13))\n",
    "#rs_ma = np.mean(rs_ma, axis=0)\n",
    "#rs_simple = np.mean(rs_simple, axis=0)\n",
    "#rs_ma = rs_ma.flatten()\n",
    "#rs_simple = rs_simple.flatten()\n",
    "plt.plot(rs_ma, label='ma')\n",
    "plt.plot(rs_simple, label='simple')\n",
    "#plt.plot(rs_donothing, label='do nothing')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (21,13))\n",
    "plt.plot(np.cumsum(rs_ma) - np.cumsum(rs_simple), label='ma - simple')\n",
    "plt.hlines(y=0, xmin=0, xmax=len(rs_ma), colors='red', label='0')\n",
    "#plt.plot(rs_donothing, label='do nothing')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make(env_name, test=True, backend = bk_cls(),\n",
    "                action_class=PlayableAction, _add_to_name=\"_test_ma\")\n",
    "\n",
    "\n",
    "ma_env = MultiAgentEnv(env, action_domains, copy_env=False)\n",
    "\n",
    "#env = ma_env._cent_env \n",
    "ma_env.seed(0)\n",
    "ma_env._cent_env.set_id(0)\n",
    "obs = ma_env.reset()\n",
    "i = 0\n",
    "rewards = []\n",
    "\n",
    "#from grid2op.PlotGrid import PlotMatplot\n",
    "#plot_helper = PlotMatplot(env.observation_space)\n",
    "#_ = plot_helper.plot_obs(obs['agent_0'])\n",
    "\n",
    "while True:\n",
    "    action = results_ma['actions'][0][i]\n",
    "    i += 1\n",
    "    new_obs, reward, done, info = ma_env._cent_env.step(action)\n",
    "    rewards.append(reward)\n",
    "    if done :\n",
    "        plt.plot(rewards, label = 'replay')\n",
    "        plt.plot(results_ma['rewards'][0], label = 'ma')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.plot(np.array(rewards) - np.array(results_ma['rewards'][0]))\n",
    "        assert (np.array(rewards) - np.array(results_ma['rewards'][0]) == 0).all()\n",
    "        #_ = plot_helper.plot_obs(obs)\n",
    "        break\n",
    "    obs = new_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = results_ma['actions'][0][i]\n",
    "print(action)\n",
    "i+=1\n",
    "obs, reward, done, info = env.step(action)\n",
    "_ = plot_helper.plot_obs(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7add5ee7d9634725b67ead71c805f149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a41c9991964951b6ad80d05056d166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-12\n",
      "\t\t - cumulative reward: 4934.723633\n",
      "\t\t - number of time steps completed: 288 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a8e12597f0d44b5bd9402a06e5c2d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b2d0eb072794fbcb20f922c567b3c81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-13\n",
      "\t\t - cumulative reward: 4915.105957\n",
      "\t\t - number of time steps completed: 288 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38cfa42b43a4084a636a4bb041ff6ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1605d9e41c574a0885d9dfba8dd42fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-14\n",
      "\t\t - cumulative reward: 3727.450439\n",
      "\t\t - number of time steps completed: 231 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8badfa00b794d148e27e97fc3b92e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a0862d7eb1b403c859034b719d70086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-15\n",
      "\t\t - cumulative reward: 4600.995117\n",
      "\t\t - number of time steps completed: 288 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef3cfd67bc74eef93ab36d7612de9cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c5b03fce83446d4a710b00cec212bfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-16\n",
      "\t\t - cumulative reward: 2430.205811\n",
      "\t\t - number of time steps completed: 147 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "594ffcbdb17b43b88c9814a5f9835b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d5e4c7640664667b57332b2b7661b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-17\n",
      "\t\t - cumulative reward: 1808.123657\n",
      "\t\t - number of time steps completed: 109 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a1eb7fe5be49a8b108ff98fa2132f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7791f76b9c9242aebcc682a41e9c8fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-18\n",
      "\t\t - cumulative reward: 1475.211914\n",
      "\t\t - number of time steps completed: 93 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af1f8e05a4bd4e2abc941cea84a7b90f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd9eac7ea314c368595d5f4d08b9a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-12\n",
      "\t\t - cumulative reward: 4934.723633\n",
      "\t\t - number of time steps completed: 288 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "452586f6924a41209df963fb4f29dfb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a2b0681b9449898ae25292666ad397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-13\n",
      "\t\t - cumulative reward: 4915.105957\n",
      "\t\t - number of time steps completed: 288 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412ea709a739483aaf90ee0ad1a9be08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3c56b414de4eb5be8e4976825bb27b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-14\n",
      "\t\t - cumulative reward: 3727.450439\n",
      "\t\t - number of time steps completed: 231 / 288\n"
     ]
    }
   ],
   "source": [
    "from grid2op.Runner import Runner\n",
    "import os \n",
    "import shutil\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "env = ma_env._cent_env \n",
    "\n",
    "path_agents = \"study_agent_getting_started\"\n",
    "max_iter = 10_000\n",
    "\n",
    "shutil.rmtree(os.path.abspath(path_agents), ignore_errors=True)\n",
    "if not os.path.exists(path_agents):\n",
    "    os.mkdir(path_agents)\n",
    "\n",
    "# make a runner for this agent\n",
    "path_agent = os.path.join(path_agents, \"ReplayAgent\")\n",
    "shutil.rmtree(os.path.abspath(path_agent), ignore_errors=True)\n",
    "\n",
    "env.seed(0)\n",
    "env.set_id(0)\n",
    "\n",
    "replay_cum_rewards = []\n",
    "\n",
    "for i, replay in enumerate(replays):\n",
    "\n",
    "    runner = Runner(**env.get_params_for_runner(),\n",
    "                    agentClass=None,\n",
    "                    agentInstance=replay\n",
    "                    )\n",
    "    res = runner.run(path_save=path_agent,\n",
    "                     nb_episode=1, \n",
    "                     max_iter=max_iter,\n",
    "                     env_seeds=[0],\n",
    "                     episode_id=[i],\n",
    "                     pbar=tqdm)\n",
    "    print(\"The results for the evaluated agent are:\")\n",
    "    for _, chron_id, cum_reward, nb_time_step, max_ts in res:\n",
    "        replay_cum_rewards.append(cum_reward)\n",
    "        msg_tmp = \"\\tFor chronics with id {}\\n\".format(chron_id)\n",
    "        msg_tmp += \"\\t\\t - cumulative reward: {:.6f}\\n\".format(cum_reward)\n",
    "        msg_tmp += \"\\t\\t - number of time steps completed: {:.0f} / {:.0f}\".format(nb_time_step, max_ts)\n",
    "        print(msg_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21bfd3e321f64975baf271fdb1e2720e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca40c6aaaae43d18d2d5f2aee5eedbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-12\n",
      "\t\t - cumulative reward: 4934.622070\n",
      "\t\t - number of time steps completed: 288 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a2289266f14d1593dc5691a84b35a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9b352029484471a890a27d56541402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-13\n",
      "\t\t - cumulative reward: 4914.632812\n",
      "\t\t - number of time steps completed: 288 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664ed3c5142048e1a9a4556d0f50ac55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93152bfcc8e8420f9b4947647781f5b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-14\n",
      "\t\t - cumulative reward: 3719.424561\n",
      "\t\t - number of time steps completed: 230 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67b108a09ab7497cbdfd9dd4aed206ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0629ba57868441495818b79e5419f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-15\n",
      "\t\t - cumulative reward: 4600.714355\n",
      "\t\t - number of time steps completed: 288 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d18c131b41ae4814a253bfd4d3798252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d10b4d3e622c494fb9c578300cb2ecc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-16\n",
      "\t\t - cumulative reward: 3964.305908\n",
      "\t\t - number of time steps completed: 241 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3089c85ea534a65b21d10cbcc3f9301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da2b5c842d446519ebd9670ae41d48d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-17\n",
      "\t\t - cumulative reward: 1807.971436\n",
      "\t\t - number of time steps completed: 109 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f764647e53b44e11afd5638aa6fad26c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fc6f2ad65a24b22abaca28889c30800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-18\n",
      "\t\t - cumulative reward: 1474.747925\n",
      "\t\t - number of time steps completed: 93 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "861d58285362486b99e60aae9436d0e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "483721b558db41f9ae26aa2ad3f92040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-12\n",
      "\t\t - cumulative reward: 4934.622070\n",
      "\t\t - number of time steps completed: 288 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc2ae03e30a403da9fbe53ac65d35c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4694aa2dfd094cd08af0304480438039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-13\n",
      "\t\t - cumulative reward: 4914.632812\n",
      "\t\t - number of time steps completed: 288 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d595f19a954e039f10ef91e57c8ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c06925c8a0402b9873d72b01a73121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-14\n",
      "\t\t - cumulative reward: 3719.424561\n",
      "\t\t - number of time steps completed: 230 / 288\n"
     ]
    }
   ],
   "source": [
    "env = ma_env._cent_env \n",
    "\n",
    "path_agents = \"study_agent_getting_started\"\n",
    "max_iter = 10_000\n",
    "\n",
    "shutil.rmtree(os.path.abspath(path_agents), ignore_errors=True)\n",
    "if not os.path.exists(path_agents):\n",
    "    os.mkdir(path_agents)\n",
    "\n",
    "# make a runner for this agent\n",
    "path_agent = os.path.join(path_agents, \"ReplayAgent\")\n",
    "shutil.rmtree(os.path.abspath(path_agent), ignore_errors=True)\n",
    "\n",
    "env.seed(0)\n",
    "env.set_id(0)\n",
    "\n",
    "replay_simple_cum_rewards = []\n",
    "\n",
    "for i, replay in enumerate(one_agent_replays):\n",
    "\n",
    "    runner = Runner(**env.get_params_for_runner(),\n",
    "                    agentClass=None,\n",
    "                    agentInstance=replay\n",
    "                    )\n",
    "    res = runner.run(path_save=path_agent,\n",
    "                     nb_episode=1, \n",
    "                     max_iter=max_iter,\n",
    "                     env_seeds=[0],\n",
    "                     episode_id=[i],\n",
    "                     pbar=tqdm)\n",
    "    print(\"The results for the evaluated agent are:\")\n",
    "    for _, chron_id, cum_reward, nb_time_step, max_ts in res:\n",
    "        replay_simple_cum_rewards.append(cum_reward)\n",
    "        msg_tmp = \"\\tFor chronics with id {}\\n\".format(chron_id)\n",
    "        msg_tmp += \"\\t\\t - cumulative reward: {:.6f}\\n\".format(cum_reward)\n",
    "        msg_tmp += \"\\t\\t - number of time steps completed: {:.0f} / {:.0f}\".format(nb_time_step, max_ts)\n",
    "        print(msg_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcd672230d0>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(results_ma['rewards'][9])\n",
    "plt.plot(results_simple['rewards'][9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9370f5a16941c0ace13b869eedf17d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b72a25724449f0807f73e56203f179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-12\n",
      "\t\t - cumulative reward: 4688.167480\n",
      "\t\t - number of time steps completed: 288 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0cc7d0f10ff48baae36b2056f251704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d277ffde041a48e49525bd261913b569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-13\n",
      "\t\t - cumulative reward: 4685.237305\n",
      "\t\t - number of time steps completed: 288 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adffaa63f34e42139b470a60991b9c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac880a13259740d09145085abdd26d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-14\n",
      "\t\t - cumulative reward: 3344.289551\n",
      "\t\t - number of time steps completed: 220 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c96acd393c56459cbb3eb748e4833fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d0e1e6e7344dafa3db75517acf807d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-15\n",
      "\t\t - cumulative reward: 1388.156250\n",
      "\t\t - number of time steps completed: 90 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae395cd2be24e09b976fa88ff9b526d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e37841945e7145f68b08eb91fc3aa252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-16\n",
      "\t\t - cumulative reward: 3623.987061\n",
      "\t\t - number of time steps completed: 236 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf34e307b1a40e8a3be77d4dd35557e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a4915c5ca74da08ff7741afc34865d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-17\n",
      "\t\t - cumulative reward: 3546.453125\n",
      "\t\t - number of time steps completed: 234 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d15cb9e0990247b1a4aa3b3de7aed36a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb53350222d4765b71592897ebbd46e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-18\n",
      "\t\t - cumulative reward: 1281.684448\n",
      "\t\t - number of time steps completed: 86 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58209e9ae6204ecdabf24031d8984c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b35af73cadf4a4094e56cc46e7162c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-12\n",
      "\t\t - cumulative reward: 4688.167480\n",
      "\t\t - number of time steps completed: 288 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba0658549644c2d8dade729e78e64dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb195201a87b4e89b6f5fdee27a3658a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-13\n",
      "\t\t - cumulative reward: 4685.237305\n",
      "\t\t - number of time steps completed: 288 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca3ddec2f0d24b2ebcc32f9473a2aaeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac54a17e7e744e6092aec7888f49688f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-14\n",
      "\t\t - cumulative reward: 3344.289551\n",
      "\t\t - number of time steps completed: 220 / 288\n"
     ]
    }
   ],
   "source": [
    "from grid2op.Agent import DoNothingAgent\n",
    "env = ma_env._cent_env \n",
    "\n",
    "path_agents = \"study_agent_getting_started\"\n",
    "max_iter = 10_000\n",
    "\n",
    "shutil.rmtree(os.path.abspath(path_agents), ignore_errors=True)\n",
    "if not os.path.exists(path_agents):\n",
    "    os.mkdir(path_agents)\n",
    "\n",
    "# make a runner for this agent\n",
    "path_agent = os.path.join(path_agents, \"ReplayAgent\")\n",
    "shutil.rmtree(os.path.abspath(path_agent), ignore_errors=True)\n",
    "\n",
    "env.seed(0)\n",
    "env.set_id(0)\n",
    "\n",
    "replay_simple_cum_rewards = []\n",
    "\n",
    "for i, replay in enumerate(one_agent_replays):\n",
    "\n",
    "    runner = Runner(**env.get_params_for_runner(),\n",
    "                    agentClass=DoNothingAgent,\n",
    "                    )\n",
    "    res = runner.run(path_save=path_agent,\n",
    "                     nb_episode=1, \n",
    "                     max_iter=max_iter,\n",
    "                     env_seeds=[0],\n",
    "                     episode_id=[i],\n",
    "                     pbar=tqdm)\n",
    "    print(\"The results for the evaluated agent are:\")\n",
    "    for _, chron_id, cum_reward, nb_time_step, max_ts in res:\n",
    "        replay_simple_cum_rewards.append(cum_reward)\n",
    "        msg_tmp = \"\\tFor chronics with id {}\\n\".format(chron_id)\n",
    "        msg_tmp += \"\\t\\t - cumulative reward: {:.6f}\\n\".format(cum_reward)\n",
    "        msg_tmp += \"\\t\\t - number of time steps completed: {:.0f} / {:.0f}\".format(nb_time_step, max_ts)\n",
    "        print(msg_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each agent try to predict other agent's action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained models\n",
    "import pickle\n",
    "with open('agent0_predictor_forest_l2rpn.pkl', 'rb') as f:\n",
    "    model0 = pickle.load(f)\n",
    "with open('agent1_predictor_forest_l2rpn.pkl', 'rb') as f:\n",
    "    model1 = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors10 = {\n",
    "    'agent_0' : Predictor(\n",
    "        ma_env.action_spaces['agent_1'],\n",
    "        model=model1\n",
    "    ),\n",
    "    'agent_1' : Predictor(\n",
    "        ma_env.action_spaces['agent_0'],\n",
    "        do_nothing=True\n",
    "    ),\n",
    "}\n",
    "\n",
    "predictors01 = {\n",
    "    'agent_0' : Predictor(\n",
    "        ma_env.action_spaces['agent_1'],\n",
    "        do_nothing=True\n",
    "    ),\n",
    "    'agent_1' : Predictor(\n",
    "        ma_env.action_spaces['agent_0'],\n",
    "        model=model0\n",
    "    ),\n",
    "}\n",
    "\n",
    "predictors11 = {\n",
    "    'agent_0' : Predictor(\n",
    "        ma_env.action_spaces['agent_1'],\n",
    "        model=model1\n",
    "    ),\n",
    "    'agent_1' : Predictor(\n",
    "        ma_env.action_spaces['agent_0'],\n",
    "        model=model0\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running multi-agent simulation !\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94de1c400044ba9bce157e904190253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid2op.Agent import TopologyGreedy\n",
    "\n",
    "simple_actor = TopologyGreedy(env.action_space)\n",
    "episodes = 10\n",
    "ma_actors = dict()\n",
    "for agent_nm in ma_env.agents:\n",
    "    ma_actors[agent_nm] = LocalTopologyGreedyExpert(\n",
    "        agent_nm,\n",
    "        ma_env.action_spaces[agent_nm],\n",
    "        ma_env,\n",
    "        predictors01\n",
    "    )\n",
    "    \n",
    "ma_env.seed(0)\n",
    "ma_env._cent_env.set_id(0)\n",
    "\n",
    "results_ma01 = _run_ma_actors(\n",
    "    ma_env,\n",
    "    actors=ma_actors,\n",
    "    nb_episodes=episodes,\n",
    "    # TODO plus d'episodes\n",
    ")\n",
    "np.save('results_ma_w_01predict_l2rpn.npy', arr=results_ma01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running multi-agent simulation !\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee03556fb1b14f67b27c74e8b95369c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "simple_actor = TopologyGreedy(env.action_space)\n",
    "episodes = 10\n",
    "ma_actors = dict()\n",
    "for agent_nm in ma_env.agents:\n",
    "    ma_actors[agent_nm] = LocalTopologyGreedyExpert(\n",
    "        agent_nm,\n",
    "        ma_env.action_spaces[agent_nm],\n",
    "        ma_env,\n",
    "        predictors10\n",
    "    )\n",
    "    \n",
    "ma_env.seed(0)\n",
    "ma_env._cent_env.set_id(0)\n",
    "\n",
    "results_ma10 = _run_ma_actors(\n",
    "    ma_env,\n",
    "    actors=ma_actors,\n",
    "    nb_episodes=episodes,\n",
    "    # TODO plus d'episodes\n",
    ")\n",
    "np.save('results_ma_w_10predict_l2rpn.npy', arr=results_ma10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running multi-agent simulation !\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c22614f59354ae0a985fd3120d5f8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "simple_actor = TopologyGreedy(env.action_space)\n",
    "episodes = 10\n",
    "ma_actors = dict()\n",
    "for agent_nm in ma_env.agents:\n",
    "    ma_actors[agent_nm] = LocalTopologyGreedyExpert(\n",
    "        agent_nm,\n",
    "        ma_env.action_spaces[agent_nm],\n",
    "        ma_env,\n",
    "        predictors11\n",
    "    )\n",
    "    \n",
    "ma_env.seed(0)\n",
    "ma_env._cent_env.set_id(0)\n",
    "\n",
    "results_ma11 = _run_ma_actors(\n",
    "    ma_env,\n",
    "    actors=ma_actors,\n",
    "    nb_episodes=episodes,\n",
    "    # TODO plus d'episodes\n",
    ")\n",
    "np.save('results_ma_w_11predict_l2rpn.npy', arr=results_ma11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('results_ma_w_predict.npy', arr=results_ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid2op.Agent.fromActionsListAgent import FromActionsListAgent\n",
    "\n",
    "one_agent_replays = []\n",
    "replays = []\n",
    "for episode in range(episodes):\n",
    "    # Faire plusieurs replay sur plusieurs episodes\n",
    "    replays.append(\n",
    "        FromActionsListAgent(\n",
    "            env.action_space,\n",
    "            results_ma['actions'][episode]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b53a92ef904250a1470cfb1ddc34a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6931e94f724762b52e346b4d78c417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-12\n",
      "\t\t - cumulative reward: 4934.969238\n",
      "\t\t - number of time steps completed: 288 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a76ab5dec443debe2d5d2d73244b0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652a8d961c09481b84d5e9a7670b4329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-13\n",
      "\t\t - cumulative reward: 4915.104980\n",
      "\t\t - number of time steps completed: 288 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e367a6e369a94525abee79045fc3fe12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fbb728049a442a29694f2db693e40ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-14\n",
      "\t\t - cumulative reward: 3661.568115\n",
      "\t\t - number of time steps completed: 226 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b960b9468abe43a09dccc4f850c99821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "281d5376280547a0bec2fb5ccf13fa3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-15\n",
      "\t\t - cumulative reward: 4600.977539\n",
      "\t\t - number of time steps completed: 288 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c28ec1ea3e4964bd6a09924ee6002d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c2bb8d278c4da89dee88bd7697abb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-16\n",
      "\t\t - cumulative reward: 2430.167480\n",
      "\t\t - number of time steps completed: 147 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f98c817de3b4d8fa19faf2870abc014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59cb0fa11a644ecf910862be15eb471b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-17\n",
      "\t\t - cumulative reward: 1808.123657\n",
      "\t\t - number of time steps completed: 109 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1c3b6ded034d0883416f3a4107c313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345d7dd519b64f0393b46032c4c6ad05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-18\n",
      "\t\t - cumulative reward: 1475.211914\n",
      "\t\t - number of time steps completed: 93 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23d6c97ad714b39a62c7285188fc7ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6fc10925884c4ead526b96be03c49f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-12\n",
      "\t\t - cumulative reward: 4934.969238\n",
      "\t\t - number of time steps completed: 288 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f7f35d74a04240b9071de74a3e1a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ea40fccdb14d93bfd4ed4dcfc0619f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-13\n",
      "\t\t - cumulative reward: 4915.104980\n",
      "\t\t - number of time steps completed: 288 / 288\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39bfab07a27468d9fe1e329e79d64c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73e19a40d7d5402890a4bb46f0fd3664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/288 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 2019-01-14\n",
      "\t\t - cumulative reward: 3661.568115\n",
      "\t\t - number of time steps completed: 226 / 288\n"
     ]
    }
   ],
   "source": [
    "from grid2op.Runner import Runner\n",
    "import os \n",
    "import shutil\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "env = ma_env._cent_env \n",
    "\n",
    "path_agents = \"study_agent_getting_started\"\n",
    "max_iter = 10_000\n",
    "\n",
    "shutil.rmtree(os.path.abspath(path_agents), ignore_errors=True)\n",
    "if not os.path.exists(path_agents):\n",
    "    os.mkdir(path_agents)\n",
    "\n",
    "# make a runner for this agent\n",
    "path_agent = os.path.join(path_agents, \"ReplayAgent\")\n",
    "shutil.rmtree(os.path.abspath(path_agent), ignore_errors=True)\n",
    "\n",
    "env.seed(0)\n",
    "env.set_id(0)\n",
    "\n",
    "replay_cum_rewards = []\n",
    "\n",
    "for i, replay in enumerate(replays):\n",
    "\n",
    "    runner = Runner(**env.get_params_for_runner(),\n",
    "                    agentClass=None,\n",
    "                    agentInstance=replay\n",
    "                    )\n",
    "    res = runner.run(path_save=path_agent,\n",
    "                     nb_episode=1, \n",
    "                     max_iter=max_iter,\n",
    "                     env_seeds=[0],\n",
    "                     episode_id=[i],\n",
    "                     pbar=tqdm)\n",
    "    print(\"The results for the evaluated agent are:\")\n",
    "    for _, chron_id, cum_reward, nb_time_step, max_ts in res:\n",
    "        replay_cum_rewards.append(cum_reward)\n",
    "        msg_tmp = \"\\tFor chronics with id {}\\n\".format(chron_id)\n",
    "        msg_tmp += \"\\t\\t - cumulative reward: {:.6f}\\n\".format(cum_reward)\n",
    "        msg_tmp += \"\\t\\t - number of time steps completed: {:.0f} / {:.0f}\".format(nb_time_step, max_ts)\n",
    "        print(msg_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e9695fee56864081dd9787bed9cb2ecf5768f301e19b28b0d4bc6bbab594eacc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('grid2op')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
