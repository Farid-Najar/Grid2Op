{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/najarfar/Internship/Grid2Op/grid2op/MakeEnv/Make.py:394: UserWarning: You are using a development environment. This environment is not intended for training agents. It might not be up to date and its primary use if for tests (hence the \"test=True\" you passed as argument). Use at your own risk.\n",
      "  warnings.warn(_MAKE_DEV_ENV_WARN)\n",
      "/home/najarfar/Internship/Grid2Op/grid2op/multi_agent/multiAgentEnv.py:123: UserWarning: Rules can not be changed in this version.\n",
      "  warnings.warn(\"Rules can not be changed in this version.\")\n",
      "/home/najarfar/Internship/Grid2Op/grid2op/multi_agent/multiAgentEnv.py:129: UserWarning: The central env has been heavily modified (parameters and reset) !\n",
      "  warnings.warn(\"The central env has been heavily modified (parameters and reset) !\")\n"
     ]
    }
   ],
   "source": [
    "from grid2op import make\n",
    "from grid2op.Action.PlayableAction import PlayableAction\n",
    "from grid2op.multi_agent.multiAgentEnv import MultiAgentEnv\n",
    "import numpy as np\n",
    "from grid2op.multi_agent.multi_agentExceptions import *\n",
    "\n",
    "from lightsim2grid import LightSimBackend\n",
    "bk_cls = LightSimBackend\n",
    "\n",
    "action_domains = {\n",
    "    'agent_0' : [0,1,2,3, 4],\n",
    "    'agent_1' : [5,6,7,8,9,10,11,12,13]\n",
    "}\n",
    "env_name = \"l2rpn_case14_sandbox\"#\"educ_case14_storage\"\n",
    "env = make(env_name, test=True, backend=bk_cls(),\n",
    "                action_class=PlayableAction, _add_to_name=\"_test_ma\", )\n",
    "\n",
    "\n",
    "ma_env = MultiAgentEnv(env, action_domains, copy_env=False)\n",
    "\n",
    "ma_env.seed(0)\n",
    "obs = ma_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(467,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "o = np.array(obs.to_vect())\n",
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = {1 : 1}\n",
    "np.save('test.npy', arr=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.load('test.npy', allow_pickle=True)\n",
    "type(test[None][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from grid2op.multi_agent.ma_typing import MAAgents\n",
    "from grid2op.Environment.BaseEnv import BaseEnv\n",
    "from grid2op.Agent.baseAgent import BaseAgent\n",
    "from grid2op.multi_agent.multiAgentEnv import MultiAgentEnv\n",
    "\n",
    "\n",
    "def _run_simple_actor(\n",
    "    env : BaseEnv,\n",
    "    actor : BaseAgent,\n",
    "    nb_episodes : int,\n",
    ") -> dict:\n",
    "    \n",
    "    T = np.zeros(nb_episodes, dtype = int)\n",
    "    obs = env.reset()\n",
    "    t = 0\n",
    "    \n",
    "    rewards_history = []\n",
    "    mean_rewards_history = np.zeros(nb_episodes)\n",
    "    std_rewards_history = np.zeros(nb_episodes)\n",
    "    cumulative_reward = np.zeros(nb_episodes)\n",
    "    \n",
    "    info_history = [[] for _ in range(nb_episodes)]\n",
    "    \n",
    "    obs_history = [[] for _ in range(nb_episodes)]\n",
    "    \n",
    "    done_history = [[] for _ in range(nb_episodes)]\n",
    "    \n",
    "    actions_history = [[] for _ in range(nb_episodes)]\n",
    "    \n",
    "    reward = 0\n",
    "    \n",
    "    for episode in range(nb_episodes):\n",
    "        while True:\n",
    "            t += 1\n",
    "            action = actor.act(observation = obs, reward = reward)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            \n",
    "            #obs._obs_env = None\n",
    "\n",
    "            rewards_history.append(reward)\n",
    "            info_history[episode].append(info.copy())\n",
    "            obs_history[episode].append(obs)\n",
    "            done_history[episode].append(done)\n",
    "            actions_history[episode].append(action)\n",
    "            \n",
    "\n",
    "            if done:\n",
    "                mean_rewards_history[episode] = np.mean(rewards_history)\n",
    "                std_rewards_history[episode] = np.std(rewards_history)\n",
    "                cumulative_reward[episode] = np.sum(rewards_history)\n",
    "                obs = env.reset()\n",
    "                T[episode] = t\n",
    "                t = 0\n",
    "                break\n",
    "            \n",
    "    return {\n",
    "        'mean_rewards' : mean_rewards_history,\n",
    "        'std_rewards' : std_rewards_history,\n",
    "        'episode_len' : T,\n",
    "        'info_history' : info_history,\n",
    "        'obs_history' : obs_history,\n",
    "        'done_history' : done_history,\n",
    "        'actions' : actions_history,\n",
    "        'cumulative_reward' : cumulative_reward\n",
    "         \n",
    "        # TODO cum reward done\n",
    "        # TODO local actions\n",
    "    }\n",
    "\n",
    "def _run_ma_actors(\n",
    "    ma_env : MultiAgentEnv,\n",
    "    actors : MAAgents,\n",
    "    nb_episodes : int,\n",
    ") -> dict:\n",
    "    \n",
    "    T = np.zeros(nb_episodes, dtype = int)\n",
    "    obs = ma_env.reset()\n",
    "    t = 0\n",
    "    \n",
    "    rewards_history = []\n",
    "    mean_rewards_history = np.zeros(nb_episodes)\n",
    "    std_rewards_history = np.zeros(nb_episodes)\n",
    "    cumulative_reward = np.zeros(nb_episodes)\n",
    "    \n",
    "    info_history = [[] for _ in range(nb_episodes)]\n",
    "    \n",
    "    obs_history = [[] for _ in range(nb_episodes)]\n",
    "    \n",
    "    done_history = [[] for _ in range(nb_episodes)]\n",
    "    \n",
    "    actions_history = [[] for _ in range(nb_episodes)]\n",
    "    \n",
    "    r = 0\n",
    "    \n",
    "    for episode in range(nb_episodes):\n",
    "        while True:\n",
    "            t += 1\n",
    "            actions = {\n",
    "                agent : actors[agent].act(observation = obs[agent], reward = r)\n",
    "                for agent in ma_env.agents\n",
    "            }\n",
    "            obs, reward, dones, info = ma_env.step(actions)\n",
    "\n",
    "            r = reward[ma_env.agents[0]]\n",
    "            rewards_history.append(r)\n",
    "            info_history[episode].append(info[ma_env.agents[0]].copy())\n",
    "            \n",
    "            for agent in ma_env.agents:\n",
    "                # TODO pourquoi ce probl√®me ?\n",
    "                obs[agent]._obs_env = None\n",
    "                \n",
    "            obs_history[episode].append(obs[ma_env.agents[0]])\n",
    "            done_history[episode].append(dones[ma_env.agents[0]])\n",
    "            actions_history[episode].append(ma_env.global_action)\n",
    "                \n",
    "\n",
    "            if dones[ma_env.agents[0]]:\n",
    "                mean_rewards_history[episode] = np.mean(rewards_history)\n",
    "                std_rewards_history[episode] =  np.std(rewards_history)\n",
    "                cumulative_reward[episode] = np.sum(rewards_history)\n",
    "                \n",
    "                rewards_history = []\n",
    "                \n",
    "                obs = ma_env.reset()\n",
    "                T[episode] = t\n",
    "                t = 0\n",
    "                break\n",
    "            \n",
    "    return {\n",
    "        'mean_rewards' : mean_rewards_history,\n",
    "        'std_rewards' : std_rewards_history,\n",
    "        'episode_len' : T,\n",
    "        'info_history' : info_history,\n",
    "        'obs_history' : obs_history,\n",
    "        'done_history' : done_history,\n",
    "        'actions' : actions_history,\n",
    "        'cumulative_reward' : cumulative_reward\n",
    "    }\n",
    "\n",
    "    \n",
    "def compare_simple_and_multi(\n",
    "    env : BaseEnv, # It is grid2op.multi_agent.multiAgentEnv.MultiAgentEnv\n",
    "    simple_actor : BaseAgent, \n",
    "    ma_actors : MAAgents, \n",
    "    episodes : int = 2,\n",
    "    seed = 0,\n",
    "    chronics_id = 0,\n",
    "    save_path = \"./\",\n",
    "    copy_env = True\n",
    "    ):\n",
    "    \n",
    "    ma_env = MultiAgentEnv(env, action_domains, copy_env=copy_env)\n",
    "    \n",
    "    ma_env.seed(seed)\n",
    "    ma_env._cent_env.set_id(chronics_id)\n",
    "    \n",
    "    results_simple = _run_simple_actor(ma_env._cent_env, simple_actor, episodes)\n",
    "    results_ma = _run_ma_actors(ma_env, ma_actors, episodes)\n",
    "    \n",
    "    #save results\n",
    "    # TODO\n",
    "    \n",
    "    return results_simple, results_ma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay with 2 random agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/najarfar/Internship/Grid2Op/grid2op/multi_agent/multiAgentEnv.py:123: UserWarning: Rules can not be changed in this version.\n",
      "  warnings.warn(\"Rules can not be changed in this version.\")\n",
      "/home/najarfar/Internship/Grid2Op/grid2op/multi_agent/multiAgentEnv.py:129: UserWarning: The central env has been heavily modified (parameters and reset) !\n",
      "  warnings.warn(\"The central env has been heavily modified (parameters and reset) !\")\n"
     ]
    }
   ],
   "source": [
    "from grid2op.Agent import RandomAgent\n",
    "from grid2op.Converter.IdToAct import IdToAct\n",
    "\n",
    "simple_actor = RandomAgent(env.action_space)\n",
    "episodes = 5\n",
    "ma_actors = dict()\n",
    "for agent_nm in ma_env.agents:\n",
    "    IdToActThis = ma_env.action_spaces[agent_nm].make_local(IdToAct)\n",
    "    #assert IdToActThis.agent_name == agent_nm    \n",
    "    ma_actors[agent_nm] = RandomAgent(ma_env.action_spaces[agent_nm],\n",
    "                                   action_space_converter=IdToActThis\n",
    "                                   )\n",
    "\n",
    "\n",
    "results_simple, results_ma = compare_simple_and_multi(\n",
    "    env=env,\n",
    "    simple_actor=simple_actor,\n",
    "    ma_actors=ma_actors,\n",
    "    episodes=episodes,\n",
    "    copy_env=False\n",
    "    # TODO plus d'episodes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This action will:\n",
      "\t - NOT change anything to the injections\n",
      "\t - NOT perform any redispatching action\n",
      "\t - NOT modify any storage capacity\n",
      "\t - NOT perform any curtailment\n",
      "\t - NOT force any line status\n",
      "\t - NOT switch any line status\n",
      "\t - NOT switch anything in the topology\n",
      "\t - Set the bus of the following element(s):\n",
      "\t \t - Assign bus 2 to line (extremity) id 3 [on substation 3]\n",
      "\t \t - Assign bus 1 to line (extremity) id 5 [on substation 3]\n",
      "\t \t - Assign bus 2 to line (origin) id 6 [on substation 3]\n",
      "\t \t - Assign bus 1 to line (origin) id 15 [on substation 3]\n",
      "\t \t - Assign bus 1 to line (origin) id 16 [on substation 3]\n",
      "\t \t - Assign bus 2 to load id 2 [on substation 3]\n",
      "\t \t - Assign bus 2 to line (origin) id 10 [on substation 8]\n",
      "\t \t - Assign bus 1 to line (origin) id 11 [on substation 8]\n",
      "\t \t - Assign bus 1 to line (extremity) id 16 [on substation 8]\n",
      "\t \t - Assign bus 2 to line (origin) id 19 [on substation 8]\n",
      "\t \t - Assign bus 1 to load id 5 [on substation 8]\n",
      "This action will:\n",
      "\t - NOT change anything to the injections\n",
      "\t - NOT perform any redispatching action\n",
      "\t - NOT modify any storage capacity\n",
      "\t - NOT perform any curtailment\n",
      "\t - NOT force any line status\n",
      "\t - NOT switch any line status\n",
      "\t - NOT switch anything in the topology\n",
      "\t - Set the bus of the following element(s):\n",
      "\t \t - Assign bus 2 to line (extremity) id 3 [on substation 3]\n",
      "\t \t - Assign bus 2 to line (extremity) id 5 [on substation 3]\n",
      "\t \t - Assign bus 1 to line (origin) id 6 [on substation 3]\n",
      "\t \t - Assign bus 1 to line (origin) id 15 [on substation 3]\n",
      "\t \t - Assign bus 2 to line (origin) id 16 [on substation 3]\n",
      "\t \t - Assign bus 2 to load id 2 [on substation 3]\n",
      "\t \t - Assign bus 2 to line (extremity) id 9 [on substation 12]\n",
      "\t \t - Assign bus 1 to line (extremity) id 13 [on substation 12]\n",
      "\t \t - Assign bus 1 to line (origin) id 14 [on substation 12]\n",
      "\t \t - Assign bus 1 to load id 9 [on substation 12]\n"
     ]
    }
   ],
   "source": [
    "from grid2op.Agent.fromActionsListAgent import FromActionsListAgent\n",
    "\n",
    "for action in results_ma['actions'][0]:\n",
    "    print(action)\n",
    "\n",
    "replays = []\n",
    "for episode in range(episodes):\n",
    "    # Faire plusieurs replay sur plusieurs episodes\n",
    "    replays.append(\n",
    "        FromActionsListAgent(\n",
    "            env.action_space,\n",
    "            results_ma['actions'][episode]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eb12e1878e8465795787d8951b1ce48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f360e43ecb24e88b55bf199fc8d67ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 0000\n",
      "\t\t - cumulative reward: 53.410580\n",
      "\t\t - number of time steps completed: 2 / 575\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "496634e121b64ace9454ad151458c3b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "023dcbe643b74e5e9923b75f7fa20632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 0001\n",
      "\t\t - cumulative reward: 54.976692\n",
      "\t\t - number of time steps completed: 2 / 575\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8731109ef44a47cb801db1629e0a62a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9feab8ad38704565b9b5aef5bdfeab2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 0002\n",
      "\t\t - cumulative reward: -10.000000\n",
      "\t\t - number of time steps completed: 1 / 575\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22fa9ffdeec743e0bfc28e685984224a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e5294cf112146fd898e48b2cb6229f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 0000\n",
      "\t\t - cumulative reward: 117.280548\n",
      "\t\t - number of time steps completed: 3 / 575\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86fb40f1a64449408e9e2076e543ff10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c88594ca59c4f10817bccecaf15cfb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "episode:   0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the evaluated agent are:\n",
      "\tFor chronics with id 0001\n",
      "\t\t - cumulative reward: 119.353088\n",
      "\t\t - number of time steps completed: 3 / 575\n"
     ]
    }
   ],
   "source": [
    "from grid2op.Runner import Runner\n",
    "import os \n",
    "import shutil\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "env = ma_env._cent_env \n",
    "\n",
    "path_agents = \"study_agent_getting_started\"\n",
    "max_iter = 10_000\n",
    "\n",
    "shutil.rmtree(os.path.abspath(path_agents), ignore_errors=True)\n",
    "if not os.path.exists(path_agents):\n",
    "    os.mkdir(path_agents)\n",
    "\n",
    "# make a runner for this agent\n",
    "path_agent = os.path.join(path_agents, \"ReplayAgent\")\n",
    "shutil.rmtree(os.path.abspath(path_agent), ignore_errors=True)\n",
    "\n",
    "env.seed(0)\n",
    "env.set_id(0)\n",
    "env.reset()\n",
    "\n",
    "replay_cum_rewards = []\n",
    "\n",
    "for i, replay in enumerate(replays):\n",
    "\n",
    "    runner = Runner(**env.get_params_for_runner(),\n",
    "                    agentClass=None,\n",
    "                    agentInstance=replay\n",
    "                    )\n",
    "    res = runner.run(path_save=path_agent,\n",
    "                     nb_episode=1, \n",
    "                     max_iter=max_iter,\n",
    "                     env_seeds=[0],\n",
    "                     episode_id=[i],\n",
    "                     pbar=tqdm)\n",
    "    print(\"The results for the evaluated agent are:\")\n",
    "    for _, chron_id, cum_reward, nb_time_step, max_ts in res:\n",
    "        replay_cum_rewards.append(cum_reward)\n",
    "        msg_tmp = \"\\tFor chronics with id {}\\n\".format(chron_id)\n",
    "        msg_tmp += \"\\t\\t - cumulative reward: {:.6f}\\n\".format(cum_reward)\n",
    "        msg_tmp += \"\\t\\t - number of time steps completed: {:.0f} / {:.0f}\".format(nb_time_step, max_ts)\n",
    "        print(msg_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (results_ma['cumulative_reward'] == replay_cum_rewards).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e9695fee56864081dd9787bed9cb2ecf5768f301e19b28b0d4bc6bbab594eacc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('grid2op')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
